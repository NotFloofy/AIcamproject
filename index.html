<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
  <title>Mobile AI Object Detector</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <style>
    body { 
      text-align: center; 
      font-family: Arial, sans-serif; 
      margin: 0; 
      overflow: hidden; 
      background-color: #ffe6f0; 
    }
    #container { 
      position: relative; 
      width: 100vw; 
      height: 100vh; 
    }
    video { 
      width: 100%; 
      height: 100%; 
      object-fit: cover; 
      display: none; 
    }
    canvas { 
      position: absolute; 
      top: 0; 
      left: 0; 
      width: 100%; 
      height: 100%; 
      display: none; 
    }
    #status { 
      position: absolute; 
      top: 10px; 
      left: 50%; 
      transform: translateX(-50%);
      background: rgba(0, 0, 0, 0.7); 
      color: white; 
      padding: 8px 12px; 
      border-radius: 5px; 
      z-index: 2;
      font-size: 18px;
    }
    #startButton {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      font-size: 18px;
      padding: 10px 20px;
      border: none;
      background-color: #ff99cc;
      color: white;
      border-radius: 8px;
      cursor: pointer;
      z-index: 2;
    }
  </style>
</head>
<body>
  <div id="container">
    <p id="status">Tap the button to allow camera access, cutie~</p>
    <button id="startButton">Start Camera :3</button>
    <video id="webcam" autoplay playsinline></video>
    <canvas id="canvas"></canvas>
  </div>
  <script>
    const startButton = document.getElementById("startButton");
    const video = document.getElementById("webcam");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");
    const statusText = document.getElementById("status");
    let model;
    startButton.addEventListener("click", () => {
      navigator.mediaDevices.getUserMedia({ video: { facingMode: "environment", width: { ideal: 1280 }, height: { ideal: 720 } } })
        .then(stream => {
          video.srcObject = stream;
          video.addEventListener("loadedmetadata", () => {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            startButton.style.display = "none";
            video.style.display = "block";
            canvas.style.display = "block";
            statusText.innerText = "Camera is on! Loading our AI model, cutie~";
            loadModel();
          });
        })
        .catch(err => {
          alert("Oops! Unable to access the camera: " + err);
        });
    });
    async function loadModel() {
      model = await cocoSsd.load();
      statusText.innerText = "Model loaded! Detecting objects, cutie~";
      detectObjects();
    }
    async function detectObjects() {
      const predictions = await model.detect(video);
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      const threshold = 0.1;
      predictions.forEach(prediction => {
        if (prediction.score >= threshold) {
          const [x, y, width, height] = prediction.bbox;
          ctx.beginPath();
          ctx.rect(x, y, width, height);
          ctx.lineWidth = 3;
          ctx.strokeStyle = "red";
          ctx.stroke();
          const text = `${prediction.class} (${Math.round(prediction.score * 100)}%)`;
          ctx.font = "24px Arial";
          ctx.fillStyle = "white";
          ctx.strokeStyle = "black";
          ctx.lineWidth = 2;
          const textX = x;
          const textY = y > 30 ? y - 10 : y + 25;
          ctx.strokeText(text, textX, textY);
          ctx.fillText(text, textX, textY);
        }
      });
      requestAnimationFrame(detectObjects);
    }
  </script>
</body>
</html>  <!-- Coded by floofy -->
